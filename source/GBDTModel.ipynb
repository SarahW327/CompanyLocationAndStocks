{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cb9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def for GBDT:\n",
    "    # place features into dataframe and target into series  \n",
    "    boston = load_boston()\n",
    "    X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "    y = pd.Series(boston.target)\n",
    "\n",
    "    # split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # max_depth refers to the number of leaves of each tree \n",
    "    # n_estimators refers to the total number of trees in the ensemble\n",
    "    # learning_rate hyperparameter scales the contribution of each tree NOTE: If you set it to a low value, \n",
    "    # you will need more trees in the ensemble to fit the training set, but the overall variance will be lower.\n",
    "\n",
    "    # best way to tune the model: https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "    \n",
    "    regressor = GradientBoostingRegressor(\n",
    "    max_depth=2,\n",
    "    n_estimators=3,\n",
    "    learning_rate=1.0\n",
    "    )\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Use staged_predict() method to measures the validation error at each stage of training \n",
    "    # (i.e. with one tree, with two trees…) to find the optimal number of trees.\n",
    "    errors = [mean_squared_error(y_test, y_pred) for y_pred in \n",
    "              regressor.staged_predict(X_test)]\n",
    "\n",
    "    best_n_estimators = np.argmin(errors)\n",
    "\n",
    "    # build and fit our model using the optimal number of trees\n",
    "    best_regressor = GradientBoostingRegressor(\n",
    "        max_depth=2,\n",
    "        n_estimators=best_n_estimators,\n",
    "        learning_rate=1.0\n",
    "    )\n",
    "    best_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Sklearn provides numerous metrics to evaluate \n",
    "    # the performance of our machine learning models.\n",
    "    # They categorize the each metric according \n",
    "    # to the problem domain which they’re applicable. \n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html <-- GO TO THIS SITE TO SEE WHICH METRICS YOU WILL USE.\n",
    "    \n",
    "    # We use the mean absolute error \n",
    "    # which can be interpreted as \n",
    "    # the average distance from \n",
    "    # our predictions and the actual values\n",
    "\n",
    "    y_pred = best_regressor.predict(X_test)\n",
    "    mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # you are going to have to take the metric(s) and store them into \n",
    "    \n",
    "    # Tomas:  including the adj. R2\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score\n",
    "        \n",
    "    from sklearn.metrics import r2_score\n",
    "    r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Tomas: correlation analysis to see how your features are correlated to each other\n",
    "    \n",
    "    # as with any regression you need to minimize the mean square error.\n",
    "    examples are at : \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "    \n",
    "    \n",
    "    # accrossed all stocks, what is the average score.\n",
    "    # what is the mean?\n",
    "    # what is the median?\n",
    "    # do we have any outliers that we need to note\n",
    "    # does this work better for same sectors?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b283cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the csv file, put it into a dataframe\n",
    "stock_df = \"<all stock data file here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd156c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have to make all the columns a number\n",
    "# what can you use to do this? I forget\n",
    "#after you have all columns as numbers, then do the following: \n",
    "\n",
    "#sort the dataframe by ticker column\n",
    "\n",
    "#prime prev_ticker with first record's ticker value in the datafrome\n",
    "\n",
    "for each record in the the dataframe: \n",
    "    move records ticker into new_ticker\n",
    "    \n",
    "    if new_ticker = prev_ticker:\n",
    "        move the ticker value to previous value\n",
    "        and...\n",
    "        move the record to the processing_dataframe\n",
    "    else: # when the new_ticker and the previous_ticker aren't the same, \n",
    "          # we have all records in the processing_df for a given ticker\n",
    "        call the function that will do the Gradient Boosting Decision Tree Algorithm (GBDT)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
