{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a5749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#import necessary libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "#from joblib import Parallel, delayed, Model \n",
    "#from collections import Counter <--????\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b87b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GET Tabled input\n",
    "\n",
    "# creating database engine\n",
    "db_name = 'Company_Stock_DB'\n",
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/{db_name}\"\n",
    "engine = create_engine(db_string)\n",
    "\n",
    "# read data from PostgreSQL database table and load into Dataframe instance\n",
    "stock_df = pd.read_sql(\"select * from \\\"company_all_star\\\"\", engine);\n",
    "\n",
    "#sort the dataframe by ticker column\n",
    "stock_df.sort_values(by=['ticker'])\n",
    "\n",
    "# Print the DataFrame\n",
    "stock_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preserve date column as type object\n",
    "stock_df['date'] = stock_df['date_val']\n",
    "\n",
    "# have the user enter beginning date as yyyy-mm-dd\n",
    "begin_date = '2022-03-03'\n",
    "# have the user enter ending date as yyyy-mm-dd\n",
    "end_date = '2022-03-10'\n",
    "# iteration controls\n",
    "day_range_of_iter = 4\n",
    "\n",
    "# Convert the date to datetime64\n",
    "stock_df['date_val'] = pd.to_datetime(stock_df['date_val'], format='%Y-%m-%d')\n",
    "\n",
    "stock_df = stock_df.loc[(stock_df['date_val'] >= begin_date)\n",
    "                     & (stock_df['date_val'] <= end_date)]\n",
    "\n",
    "# drop throw-aways \n",
    "stock_df.drop([\"longitude\", \"latitude\", \"company_name\", \"company_url\",\"date_val\"], axis=1, inplace=True)\n",
    "\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fields that will not be used to represent a period of time\n",
    "stock_df.drop(columns = ['open_val', 'high_val', 'low_val', 'close_val', 'number_of_transactions', 'city_name', 'state_name', 'number_of_transactions', 'percent_change'], axis=1, inplace=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique days in df\n",
    "unique_days = len(pd.unique(stock_df['date']))\n",
    "print(\"unique number of days(number of days in df):\", unique_days)\n",
    "\n",
    "# unique stocks in df\n",
    "unique_stocks = len(pd.unique(stock_df['ticker']))\n",
    "print(unique_stocks)\n",
    "\n",
    "# interation sets\n",
    "iteration_sets = (unique_days - day_range_of_iter + 1)\n",
    "print(\"iteration_sets: \", iteration_sets)\n",
    "\n",
    "# total records captured\n",
    "length_of_df = len(stock_df)\n",
    "print(length_of_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012dcb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dataframe by date\n",
    "sort_date_stock_df = stock_df.sort_values(by=['date', 'ticker'])\n",
    "next_date_stock_df = sort_date_stock_df\n",
    "\n",
    "# get beginning dataframe records\n",
    "b = 0 \n",
    "# ending record for beginning df\n",
    "ending_records = iteration_sets * unique_stocks\n",
    "\n",
    "# starting record for end\n",
    "x = (unique_days - iteration_sets) * unique_stocks\n",
    "max_records = unique_days * unique_stocks\n",
    "\n",
    "begin_df = pd.DataFrame()\n",
    "end_df = pd.DataFrame()\n",
    "               \n",
    "for rec in sort_date_stock_df.iterrows():\n",
    "    \n",
    "    if b < ending_records:\n",
    "        new_begin_df = sort_date_stock_df.iloc[b]\n",
    "        begin_df = begin_df.append(new_begin_df,ignore_index=False)\n",
    "    \n",
    "    if x < max_records: \n",
    "        new_end_df = next_date_stock_df.iloc[x]\n",
    "        end_df = end_df.append(new_end_df,ignore_index=False)\n",
    "    b=b+1\n",
    "    x=x+1\n",
    "   \n",
    "begin_df.reset_index(drop=True,inplace=True)\n",
    "begin_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort dataframe by date\n",
    "# sort_date_stock_df = stock_df.sort_values(by=['date', 'ticker'])\n",
    "# sort_date_stock_df\n",
    "\n",
    "end_df.reset_index(drop=True,inplace=True)\n",
    "end_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwa_df = pd.merge(begin_df, end_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c66467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop fields that will not be used to represent a period of time\n",
    "vwa_df.drop(columns = ['date_x', 'employee_count_y', 'region_y', 'revenue_y', 'sector_y', 'ticker_y', 'country_code_y', 'date_y'], axis=1, inplace=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "vwa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ede32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwa_df['vwa'] = 100 - vwa_df['volume_weight_y']/vwa_df['volume_weight_x'] * 100\n",
    "vwa_df['va'] = 100 - vwa_df['volume_y']/vwa_df['volume_x'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1739bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values for each column (getting to know your data)\n",
    "vwa_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec74bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vwa_df.drop(columns = ['ticker_x', 'volume_x', 'volume_weight_x', 'volume_y', 'volume_weight_y'], axis=1, inplace=True)\n",
    "pd.set_option('display.max_rows', None)\n",
    "vwa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b362da",
   "metadata": {},
   "source": [
    "## Indexes, Features (the possible causes), Targets (the desired effects), Throw-Aways\n",
    "\n",
    "### NOTE: we have to keep our ticker columns (so all this must called within the gradient_boosting_decision_tree_model)\n",
    "\n",
    "#### Indexes/Primary Key: \n",
    "\n",
    "- Concatinate ticker and date to yield ticker_and_date\n",
    "\n",
    "#### Features are:\n",
    "- TICKER, \n",
    "- DATE\n",
    "- EMPLOYEE COUNT\n",
    "- REVENUE\n",
    "- SECTOR\n",
    "- COUNTRY CODE\n",
    "- VOLUME \n",
    "- VOLUME WEIGHT \n",
    "- AVERAGE_VOLUME (calculate average using begin_volumn/end_value) \n",
    "- AVERAGE_VOLUME_WEIGHT (calculate average using begin_date/end_date)\n",
    "- PERCENT CHANGE (% change from close to open)\n",
    "\n",
    "#### Target is:\n",
    "- PERCENT CHANGE (and/or) Volume Weight (???)(I think the percent change matters more because percent change yields better \n",
    "\n",
    "#### Throw-aways for modeling:\n",
    "- COMPANY NAME\n",
    "- COMPANY URL\n",
    "- CITY NAME\n",
    "- STATE NAME\n",
    "- LATITUDE\n",
    "- LONGITUDE\n",
    "- OPEN \n",
    "- HIGH \n",
    "- LOW\n",
    "- CLOSE\n",
    "- VOLUME\n",
    "- VOLUME WEIGHT\n",
    "- NUMBER OF TRANSACTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00253e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = vwa_df\n",
    "# drop stock ticker\n",
    "# filtered_df = filtered_df.drop(columns = ['city_name'])\n",
    "# filtered_df.head()\n",
    "# stock_df.drop(columns=\"ticker\", inplace=True)\n",
    "# stock_df.drop(columns=\"city_name\", inplace=True)\n",
    "# stock_df.drop(columns=\"state_name\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786cb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate our categorical variable list\n",
    "# categorical preprocessing can be done easiest using Dataframe.dtypes == 'object'\n",
    "stock_categories = stock_df.dtypes[stock_df.dtypes == \"object\"].index.tolist()\n",
    "stock_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cfc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of unique values in each column\n",
    "stock_df[stock_categories].nunique()\n",
    "# there needs to be only 10 at most in each categorie, how are we going to make this smaller...by sector ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55365b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am catagorizing my own shiza from the tiza\n",
    "# replace stock's employee count string with integer\n",
    "stock_df.loc[(stock_df['employee_count_x'] == '5k-10k'), 'employee_count_x'] = 0\n",
    "stock_df.loc[(stock_df['employee_count_x'] == 'over-10k'), 'employee_count_x'] = 1\n",
    "stock_df.loc[(stock_df['employee_count_x'] == '1k-5k'), 'employee_count_x'] = 2\n",
    "stock_df.loc[(stock_df['employee_count_x'] == '500-1k'), 'employee_count_x'] = 3\n",
    "\n",
    "# replace stock's revenue string with integer\n",
    "stock_df.loc[(stock_df['revenue_x'] == '1m-10m'), 'revenue_x'] = 0\n",
    "stock_df.loc[(stock_df['revenue_x'] == '10m-50m'), 'revenue_x'] = 1\n",
    "stock_df.loc[(stock_df['revenue_x'] == '50m-100m'), 'revenue_x'] = 2\n",
    "stock_df.loc[(stock_df['revenue_x'] == '100m-200m'), 'revenue_x'] = 3\n",
    "stock_df.loc[(stock_df['revenue_x'] == '200m-1b'), 'revenue_x'] = 4\n",
    "stock_df.loc[(stock_df['revenue_x'] == 'over-1b'), 'revenue_x'] = 5\n",
    "\n",
    "# replace stock's sector string with integer\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Technology'), 'sector_x'] = 0\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Energy'), 'sector_x'] = 1\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Healthcare'), 'sector_x'] = 2\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Consumer Discretionary'), 'sector_x'] = 3\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Industrials'), 'sector_x'] = 4\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Consumer Staples'), 'sector_x'] = 5\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Communication Services'), 'sector_x'] = 6\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Financials'), 'sector_x'] = 7\n",
    "stock_df.loc[(stock_df['sector_x'] == 'Utilities'), 'sector_x'] = 8\n",
    "\n",
    "# replace stock's country code string with integer (Note: China was CN and CH for some reason)\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'US'), 'country_code_x'] = 0\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'Netherlands'), 'country_code_x'] = 1\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'Australia'), 'country_code_x'] = 2\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'UK'), 'country_code_x'] = 3\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'CH'), 'country_code_x'] = 4\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'CN'), 'country_code_x'] = 4\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'CA'), 'country_code_x'] = 5\n",
    "stock_df.loc[(stock_df['country_code_x'] == 'Argentina'), 'country_code_x'] = 6\n",
    "\n",
    "# replace stock's region string with integer \n",
    "stock_df.loc[(stock_df['region_x'] == 'W'), 'region_x'] = 0\n",
    "stock_df.loc[(stock_df['region_x'] == 'MW'), 'region_x'] = 1\n",
    "stock_df.loc[(stock_df['region_x'] == 'SW'), 'region_x'] = 2\n",
    "stock_df.loc[(stock_df['region_x'] == 'NW'), 'region_x'] = 3\n",
    "stock_df.loc[(stock_df['region_x'] == 'SE'), 'region_x'] = 4\n",
    "stock_df.loc[(stock_df['region_x'] == 'NL'), 'region_x'] = 5\n",
    "stock_df.loc[(stock_df['region_x'] == 'AU'), 'region_x'] = 6\n",
    "stock_df.loc[(stock_df['region_x'] == 'NE'), 'region_x'] = 7\n",
    "stock_df.loc[(stock_df['region_x'] == 'GB'), 'region_x'] = 8\n",
    "stock_df.loc[(stock_df['region_x'] == 'CH'), 'region_x'] = 9\n",
    "stock_df.loc[(stock_df['region_x'] == 'CA'), 'region_x'] = 10\n",
    "\n",
    "\n",
    "#create buckets for vwa\n",
    "stock_df.loc[(stock_df['vwa'] < 0), 'vwa'] = 0\n",
    "stock_df.loc[(stock_df['vwa'] > 0) & (stock_df['vwa'] <= 1), 'vwa'] = 1\n",
    "stock_df.loc[(stock_df['vwa'] > 1) & (stock_df['vwa'] <= 2), 'vwa'] = 2\n",
    "stock_df.loc[(stock_df['vwa'] > 2) & (stock_df['vwa'] <= 3), 'vwa'] = 3\n",
    "stock_df.loc[(stock_df['vwa'] > 3) & (stock_df['vwa'] <= 4), 'vwa'] = 4\n",
    "stock_df.loc[(stock_df['vwa'] > 4) & (stock_df['vwa'] <= 5), 'vwa'] = 5\n",
    "stock_df.loc[(stock_df['vwa'] > 5) & (stock_df['vwa'] <= 6), 'vwa'] = 6\n",
    "# stock_df.loc[(stock_df['vwa'] > 6) & (stock_df['vwa'] <= 7), 'vwa'] = 7\n",
    "# stock_df.loc[(stock_df['vwa'] > 7) & (stock_df['vwa'] <= 8), 'vwa'] = 8\n",
    "# stock_df.loc[(stock_df['vwa'] > 8) & (stock_df['vwa'] <= 9), 'vwa'] = 9\n",
    "# stock_df.loc[(stock_df['vwa'] > 9) & (stock_df['vwa'] <= 10), 'vwa'] = 10\n",
    "stock_df.loc[(stock_df['vwa'] > 6), 'vwa'] = 7\n",
    "\n",
    "#create buckets for va\n",
    "stock_df.loc[(stock_df['va'] < 0), 'vwa'] = 0\n",
    "stock_df.loc[(stock_df['va'] > 0) & (stock_df['va'] <= 1), 'va'] = 1\n",
    "stock_df.loc[(stock_df['va'] > 1) & (stock_df['va'] <= 2), 'va'] = 2\n",
    "stock_df.loc[(stock_df['va'] > 2) & (stock_df['va'] <= 3), 'va'] = 3\n",
    "stock_df.loc[(stock_df['va'] > 3) & (stock_df['va'] <= 4), 'va'] = 4\n",
    "stock_df.loc[(stock_df['va'] > 4) & (stock_df['va'] <= 5), 'va'] = 5\n",
    "stock_df.loc[(stock_df['va'] > 5) & (stock_df['va'] <= 6), 'va'] = 6\n",
    "# stock_df.loc[(stock_df['va'] > 6) & (stock_df['va'] <= 7), 'va'] = 7\n",
    "# stock_df.loc[(stock_df['va'] > 7) & (stock_df['va'] <= 8), 'va'] = 8\n",
    "# stock_df.loc[(stock_df['va'] > 8) & (stock_df['va'] <= 9), 'va'] = 9\n",
    "# stock_df.loc[(stock_df['va'] > 9) & (stock_df['va'] <= 10), 'va'] = 10\n",
    "stock_df.loc[(stock_df['va'] > 6), 'va'] = 7\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2166e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check volumne weight average buckets\n",
    "vwa_counts = stock_df['vwa'].value_counts()\n",
    "vwa_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features array\n",
    "X = stock_df.drop(columns=[\"vwa\"]).values\n",
    "    \n",
    "# create target\n",
    "y = stock_df[\"vwa\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cc38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44161833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth refers to the number of leaves of each tree \n",
    "# n_estimators refers to the total number of trees in the ensemble\n",
    "# learning_rate hyperparameter scales the contribution of each tree NOTE: If you set it to a low value, \n",
    "# you will need more trees in the ensemble to fit the training set, but the overall variance will be lower.\n",
    "\n",
    "# best way to tune the model: https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "    \n",
    "regressor = GradientBoostingRegressor(\n",
    "max_depth=16,\n",
    "n_estimators=100,\n",
    "learning_rate=.01\n",
    ")\n",
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3da18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use staged_predict() method to measures the validation error at each stage of training \n",
    "# (i.e. with one tree, with two trees…) to find the optimal number of trees.\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in \n",
    "           regressor.staged_predict(X_test)]\n",
    "print(errors)\n",
    "\n",
    "# mean_squared_error: \n",
    "# The smaller the mean squared error, the closer you are to finding the line of best fit. Depending on your data, \n",
    "# it may be impossible to get a very small value for the mean squared error. For example, the above data is scattered \n",
    "# wildly around the regression line, so 6.08 is as good as it gets (and is in fact, the line of best fit). It is \n",
    "# bucketting the VWA that works. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca13e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_n_estimators = np.argmin(errors) + 1\n",
    "best_n_estimators = np.argmin(errors)\n",
    "\n",
    "print(best_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and fit our model using the optimal number of trees\n",
    "best_regressor = GradientBoostingRegressor(\n",
    "     max_depth=10,\n",
    "     n_estimators=best_n_estimators,\n",
    "     learning_rate=.01\n",
    ")\n",
    "\n",
    "best_regressor.fit(X_train, y_train)\n",
    "\n",
    "# # Sklearn provides numerous metrics to evaluate \n",
    "# # the performance of our machine learning models.\n",
    "# # They categorize the each metric according \n",
    "# # to the problem domain which they’re applicable. \n",
    "# # https://scikit-learn.org/stable/modules/model_evaluation.html <-- GO TO THIS SITE TO SEE WHICH METRICS YOU WILL USE.\n",
    "\n",
    "# # We use the mean absolute error \n",
    "# # which can be interpreted as \n",
    "# # the average distance from \n",
    "# # our predictions and the actual values\n",
    "\n",
    "# # this will give you the value of the stocks for the next period of time\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "print(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "# # this is the how well the model performed (looking for smallest error)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we be using r2_score?\n",
    "# how do you do residual plots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ec910",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2_score(y_test, y_pred))\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f775bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #     # Tomas: correlation analysis to see how your features are correlated to each other\n",
    "    \n",
    "# #     # as with any regression you need to minimize the mean square error.\n",
    "#                                                         ------------------\n",
    "# #     examples are at : \n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error\n",
    "# #     from sklearn.metrics import mean_squared_error\n",
    "    \n",
    "# # EMPTY PROCESS DATAFRAME   \n",
    "    \n",
    "# #     # accrossed all stocks, what is the average score.\n",
    "# #     # what is the mean?\n",
    "# #     # what is the median?\n",
    "# #     # do we have any outliers that we need to note\n",
    "# #     # does this work better for same sectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35900024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362cbfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
